# -*- coding: utf-8 -*-
"""ml_week04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xApHDEZtY8nq-DDQCOU-X61OaXVPgFTq
"""

import tensorflow as tf
from tensorflow.keras import layers, Model
import os
import re

from tensorflow.keras.utils import to_categorical
from sklearn.utils import shuffle


xtr = []
xte = []

ytr = []
yte = []

def load_files(f):
  x = []
  y = []
  for filedir in os.listdir(f):
    if filedir == 'neg':
      filefolder = f+'/'+filedir
      print(filefolder)
      for files in os.listdir(filefolder):
        filepath = os.path.join(filefolder, files)
        x.append(open(filepath, 'r', encoding='UTF-8').read())
        y.append(0)
    elif filedir == 'pos':
      filefolder = f+'/'+filedir
      print(filefolder)
      for files in os.listdir(filefolder):
        filepath = os.path.join(filefolder, files)
        x.append(open(filepath, 'r', encoding='UTF-8').read())
        y.append(1)

  return x, y

folder = './aclImdb/test'
xte, yte = load_files(folder)
folder = './aclImdb/train'
xtr, ytr = load_files(folder)


#print(len(xtr), len(xte))
#print(len(ytr), len(yte))
#print(xtr[1], ytr[1])
#print(xte[1], yte[1])

def preprocess(w):
  w = re.sub(r"([?.!,¿])", r" \1 ", w)
  w = re.sub(r'[" "]+', " ", w)

  w = re.sub(r"[^a-zA-Z?.!,¿]+", " ", w)
  w = w.strip()
  return w

xtrain = []
xtest = []
for s in xtr:
  xtrain.append(preprocess(s))
for s in xte:
  xtest.append(preprocess(s))


def tokenize(lang):
  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(
      filters='')
  lang_tokenizer.fit_on_texts(lang)

  tensor = lang_tokenizer.texts_to_sequences(lang)

  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,
                                                         padding='post')

  return tensor, lang_tokenizer

train_tensor, inp_lang_tokenizer = tokenize(xtrain)

test_tensor = inp_lang_tokenizer.texts_to_sequences(xtest)
test_tensor = tf.keras.preprocessing.sequence.pad_sequences(test_tensor,
                                                        padding='post')

train_label, test_label = to_categorical(ytr), to_categorical(yte)
vocab_size = len(inp_lang_tokenizer.word_index)+1

train_tensor, train_label = shuffle(train_tensor, train_label)
test_tensor, test_label = shuffle(test_tensor, test_label)
print(len(train_tensor), len(test_tensor))
print('###training###')

model = tf.keras.Sequential([
        layers.Embedding(vocab_size, 64),
        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),
        layers.Bidirectional(layers.LSTM(32)),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(2, activation='softmax')
])

model.summary()

model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy', tf.keras.metrics.AUC()])

results = model.fit(train_tensor, train_label, epochs=10, batch_size=256, validation_split = 0.2)

loss, acc, auc = model.evaluate(test_tensor, test_label)
print("test auc:", auc)

