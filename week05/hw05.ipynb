{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_week05.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP1y2TkZmXvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6c4829ad-7839-4909-a376-6623e3d321c4"
      },
      "source": [
        "cd ./drive/My\\ Drive/Colab\\ Notebooks/data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwJwIYv5mhP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget --quiet http://www.manythings.org/anki/deu-eng.zip"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSvcv9mRm6Zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip deu-eng.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS1051f2m-R9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import io\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp8JVYg5w6rJ",
        "colab_type": "text"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXJD_3KioSEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = './deu.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUJGi726oXsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "    if unicodedata.category(c) != 'Mn')\n",
        "  \n",
        "def preprocess(s):\n",
        "  s = unicode_to_ascii(s.lower().strip())\n",
        "\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1\", s)\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "\n",
        "  s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
        "  \n",
        "  s = s.strip()\n",
        "\n",
        "  s = '<start> ' + s + ' <end>'\n",
        "  return s"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek4EL_JLrDqe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3b38da7-5d47-49b1-9a5c-335236d70378"
      },
      "source": [
        "en_s = u'May I borrow this book?'\n",
        "print(preprocess(en_s))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVsgwzmaqRB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(path, num):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess(w) for w in l.split('\\t')[:2]]  for l in lines[:num]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN-l4nG7sZl7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f359d937-42e4-48e2-997c-4c15208c130c"
      },
      "source": [
        "eng, deu = create_dataset(filepath, None)\n",
        "print(eng[-1])\n",
        "print(deu[-1]) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people , and out of the few hundred that there are but a dozen or less whom he knows intimately , and out of the dozen , one or two friends at most , it will easily be seen , when we remember the number of millions who inhabit this world , that probably , since the earth was created , the right man has never yet met the right woman . <end>\n",
            "<start> ohne zweifel findet sich auf dieser welt zu jedem mann genau die richtige ehefrau und umgekehrt wenn man jedoch in betracht zieht , dass ein mensch nur gelegenheit hat , mit ein paar hundert anderen bekannt zu sein , von denen ihm nur ein dutzend oder weniger nahesteht , darunter hochstens ein oder zwei freunde , dann erahnt man eingedenk der millionen einwohner dieser welt leicht , dass seit erschaffung ebenderselben wohl noch nie der richtige mann der richtigen frau begegnet ist . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B8XnXFDsme-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "  return tensor, tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucUcpMlt0IX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num):\n",
        "  tar_lang, sour_lang = create_dataset(path, num)\n",
        "\n",
        "  tar_tensor, tar_tokenizer = tokenize(tar_lang)\n",
        "  sour_tensor, sour_tokenizer = tokenize(sour_lang)\n",
        "\n",
        "  return tar_tensor, sour_tensor, tar_tokenizer, sour_tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDfcoVqLupNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num = 50000\n",
        "y_tensor, x_tensor, y_tokenizer, x_tokenizer = load_dataset(filepath, num)\n",
        "\n",
        "x_max_length, y_max_length = x_tensor.shape[1], y_tensor.shape[1]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvvTkCu6vHmV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6f754a63-c75f-40b2-8133-874eb16e58a4"
      },
      "source": [
        "print(x_max_length, y_max_length)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf4P9VFhvRCy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a9fb194a-d88a-4bf2-96c4-6393e076da35"
      },
      "source": [
        "xtr_tensor, xte_tensor, ytr_tensor, yte_tensor = train_test_split(x_tensor, y_tensor, test_size = 0.2)\n",
        "\n",
        "print(len(xtr_tensor), len(ytr_tensor), len(xte_tensor), len(yte_tensor))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40000 40000 10000 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft67ADekuYJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib9fMMpLuZH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "906f4918-e310-4a4f-ac1b-e106176cc697"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(x_tokenizer, xtr_tensor[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(y_tokenizer, ytr_tensor[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "419 ----> bleib\n",
            "10 ----> nicht\n",
            "21 ----> zu\n",
            "323 ----> lange\n",
            "135 ----> weg\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "27 ----> don\n",
            "12 ----> t\n",
            "36 ----> be\n",
            "73 ----> too\n",
            "222 ----> long\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8X39ErJv0kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create tf.dataset\n",
        "BUFFER_SIZE = len(xtr_tensor)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(xtr_tensor)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(x_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(y_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((xtr_tensor, ytr_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdPHu2dQwz9A",
        "colab_type": "text"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUH2XyxKwzEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences = True,\n",
        "                                   return_state = True,\n",
        "                                   recurrent_initializer = 'glorot_uniform')\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqi1dny0wskN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    query_with_time_axis = tf.expand_dims(query, 1) # (batch_size, hidden size)-->(batch_size, 1, hidden size)\n",
        "\n",
        "    score = self.V(tf.nn.tanh(self.W1(query_with_time_axis)+self.W2(values)))\n",
        "\n",
        "    attention_weights = tf.nn.softmax(score, axis = 1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
        "\n",
        "    return context_vector, attention_weights\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxW_LL2l2oEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences = True,\n",
        "                                   return_state = True,\n",
        "                                   recurrent_initializer = 'glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "  \n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
        "    output, state = self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2])) # (batch_size * 1, hidden_size)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6DNGKGK56bS",
        "colab_type": "text"
      },
      "source": [
        "## optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lYhwsNo2u4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBkFHB136udy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp0-aqyU6wbw",
        "colab_type": "text"
      },
      "source": [
        "# Train and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW4n_6PW6vZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def train_step(inp, targ, enc_hidden, acc_object):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([y_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      pred = np.argmax(predictions, axis = 1)\n",
        "      acc_object.update_state(targ[:, t], pred)\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss, acc_object"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5u6u0AY64fV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "outputId": "8bd2b0d3-164c-4575-b493-46540f12a2c8"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  acc = tf.keras.metrics.Accuracy()\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss, acc = train_step(inp, targ, enc_hidden, acc)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 500 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f} Acc {:.4f}'.format(epoch + 1, batch, batch_loss.numpy(), acc.result().numpy()))\n",
        "\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f} Acc {:.4f}\\n'.format(epoch + 1, total_loss / steps_per_epoch, acc.result().numpy()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.7631 Acc 0.0000\n",
            "Epoch 1 Batch 500 Loss 1.4111 Acc 0.2594\n",
            "Epoch 1 Loss 1.8689 Acc 0.2734\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.4560 Acc 0.3500\n",
            "Epoch 2 Batch 500 Loss 1.1921 Acc 0.3663\n",
            "Epoch 2 Loss 1.2140 Acc 0.3715\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.9379 Acc 0.4172\n",
            "Epoch 3 Batch 500 Loss 0.8452 Acc 0.4159\n",
            "Epoch 3 Loss 0.8622 Acc 0.4187\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.6422 Acc 0.4500\n",
            "Epoch 4 Batch 500 Loss 0.5626 Acc 0.4591\n",
            "Epoch 4 Loss 0.6001 Acc 0.4606\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.4439 Acc 0.4875\n",
            "Epoch 5 Batch 500 Loss 0.3543 Acc 0.4963\n",
            "Epoch 5 Loss 0.4086 Acc 0.4969\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.2903 Acc 0.5406\n",
            "Epoch 6 Batch 500 Loss 0.3160 Acc 0.5251\n",
            "Epoch 6 Loss 0.2803 Acc 0.5248\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1450 Acc 0.5703\n",
            "Epoch 7 Batch 500 Loss 0.2291 Acc 0.5473\n",
            "Epoch 7 Loss 0.1947 Acc 0.5461\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1366 Acc 0.5766\n",
            "Epoch 8 Batch 500 Loss 0.2007 Acc 0.5610\n",
            "Epoch 8 Loss 0.1425 Acc 0.5600\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1106 Acc 0.5781\n",
            "Epoch 9 Batch 500 Loss 0.0984 Acc 0.5703\n",
            "Epoch 9 Loss 0.1103 Acc 0.5690\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0694 Acc 0.5797\n",
            "Epoch 10 Batch 500 Loss 0.0630 Acc 0.5752\n",
            "Epoch 10 Loss 0.0902 Acc 0.5747\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuenLjeypazv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  # sentence = preprocess(sentence)\n",
        "\n",
        "  inputs = [x_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=x_max_length, padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = '<start> '\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([y_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(y_max_length):\n",
        "    predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += y_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "    if y_tokenizer.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuDrceTHL02Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def translate(sentence):\n",
        "  result, sentence = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "  return result"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEBlyy8BL7z_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eca196be-6f29-40b2-b3fb-b6c2b1770af4"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe908c1dac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEPevGEfL8n5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "acf47dfa-a1a1-48ac-cea3-b54d661f2373"
      },
      "source": [
        "print(\"eng[True]:\", eng[2000])\n",
        "result = translate(deu[2000])\n",
        "\n",
        "score = sentence_bleu([eng[2000]], result)\n",
        "print(\"\\nBLEU score:\", score)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eng[True]: <start> are you fit ? <end>\n",
            "Input: <start> bist du in form ? <end>\n",
            "Predicted translation: <start> are you fit ? <end> \n",
            "\n",
            "BLEU score: 0.9621954581957615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu4u2vtqPztD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "cd777db5-78ba-4028-8f2d-a012f718dd26"
      },
      "source": [
        "print(\"eng[True]:\", eng[5000])\n",
        "result = translate(deu[5000])\n",
        "\n",
        "score = sentence_bleu([eng[5000]], result)\n",
        "print(\"\\nBLEU score:\", score)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eng[True]: <start> you re sharp . <end>\n",
            "Input: <start> du bist durchtrieben . <end>\n",
            "Predicted translation: <start> you re sharp . <end> \n",
            "\n",
            "BLEU score: 0.9635749534339606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lacHL7z9ZPBR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "0339517d-6567-411f-a46e-c0e0ce54aba9"
      },
      "source": [
        "print(\"eng[True]:\", eng[100000])\n",
        "result = translate(deu[100000])\n",
        "\n",
        "score = sentence_bleu([eng[100000]], result)\n",
        "print(\"\\nBLEU score:\", score)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eng[True]: <start> this knife isn t very sharp . <end>\n",
            "Input: <start> dieses messer ist nicht sehr scharf . <end>\n",
            "Predicted translation: <start> this knife is not very bad . <end> \n",
            "\n",
            "BLEU score: 0.7702389790508601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzpLBvtWQmZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "974c2d51-a5cf-45aa-8bd0-089d284c0293"
      },
      "source": [
        "print(\"eng[True]:\", eng[160000])\n",
        "result = translate(deu[160000])\n",
        "\n",
        "score = sentence_bleu([eng[160000]], result)\n",
        "print(\"\\nBLEU score:\", score)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eng[True]: <start> i thought you lived with your family . <end>\n",
            "Input: <start> ich dachte , du wohnst bei deiner familie . <end>\n",
            "Predicted translation: <start> i thought you re fine . <end> \n",
            "\n",
            "BLEU score: 0.5387398182904392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd9uTpkjXW4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}