{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_week05.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP1y2TkZmXvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8d8dc56c-6335-47ec-b430-49764922ebeb"
      },
      "source": [
        "cd ./drive/My\\ Drive/Colab\\ Notebooks/data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwJwIYv5mhP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget --quiet http://www.manythings.org/anki/deu-eng.zip"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSvcv9mRm6Zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip deu-eng.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS1051f2m-R9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import io\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp8JVYg5w6rJ",
        "colab_type": "text"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXJD_3KioSEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = './deu.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUJGi726oXsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "    if unicodedata.category(c) != 'Mn')\n",
        "  \n",
        "def preprocess(s):\n",
        "  s = unicode_to_ascii(s.lower().strip())\n",
        "\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1\", s)\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "\n",
        "  s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
        "  \n",
        "  s = s.strip()\n",
        "\n",
        "  s = '<start> ' + s + ' <end>'\n",
        "  return s"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek4EL_JLrDqe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2028f0aa-bb21-4c7b-a172-72378937b297"
      },
      "source": [
        "en_s = u'May I borrow this book?'\n",
        "print(preprocess(en_s))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVsgwzmaqRB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(path, num):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess(w) for w in l.split('\\t')[:2]]  for l in lines[:num]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN-l4nG7sZl7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6168f247-af15-4d81-9183-44f86f959b91"
      },
      "source": [
        "eng, deu = create_dataset(filepath, None)\n",
        "print(eng[-1])\n",
        "print(deu[-1]) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people , and out of the few hundred that there are but a dozen or less whom he knows intimately , and out of the dozen , one or two friends at most , it will easily be seen , when we remember the number of millions who inhabit this world , that probably , since the earth was created , the right man has never yet met the right woman . <end>\n",
            "<start> ohne zweifel findet sich auf dieser welt zu jedem mann genau die richtige ehefrau und umgekehrt wenn man jedoch in betracht zieht , dass ein mensch nur gelegenheit hat , mit ein paar hundert anderen bekannt zu sein , von denen ihm nur ein dutzend oder weniger nahesteht , darunter hochstens ein oder zwei freunde , dann erahnt man eingedenk der millionen einwohner dieser welt leicht , dass seit erschaffung ebenderselben wohl noch nie der richtige mann der richtigen frau begegnet ist . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B8XnXFDsme-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "  return tensor, tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucUcpMlt0IX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num):\n",
        "  tar_lang, sour_lang = create_dataset(path, num)\n",
        "\n",
        "  tar_tensor, tar_tokenizer = tokenize(tar_lang)\n",
        "  sour_tensor, sour_tokenizer = tokenize(sour_lang)\n",
        "\n",
        "  return tar_tensor, sour_tensor, tar_tokenizer, sour_tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDfcoVqLupNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num = 150000\n",
        "y_tensor, x_tensor, y_tokenizer, x_tokenizer = load_dataset(filepath, num)\n",
        "\n",
        "x_max_length, y_max_length = x_tensor.shape[1], y_tensor.shape[1]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvvTkCu6vHmV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6ef047cc-9c7e-47dc-f22c-4e97e9d9a37e"
      },
      "source": [
        "print(x_max_length, y_max_length)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf4P9VFhvRCy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a59ca211-c837-474e-aafd-8a97445bd8e7"
      },
      "source": [
        "xtr_tensor, xte_tensor, ytr_tensor, yte_tensor = train_test_split(x_tensor, y_tensor, test_size = 0.2)\n",
        "\n",
        "print(len(xtr_tensor), len(ytr_tensor), len(xte_tensor), len(yte_tensor))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120000 120000 30000 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft67ADekuYJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib9fMMpLuZH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "970d9119-fbb5-4346-cff0-f9ac1eb98875"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(x_tokenizer, xtr_tensor[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(y_tokenizer, ytr_tensor[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "20 ----> wir\n",
            "75 ----> gehen\n",
            "9 ----> nicht\n",
            "53 ----> aus\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "21 ----> we\n",
            "38 ----> re\n",
            "43 ----> not\n",
            "71 ----> going\n",
            "77 ----> out\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8X39ErJv0kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create tf.dataset\n",
        "BUFFER_SIZE = len(xtr_tensor)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(xtr_tensor)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(x_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(y_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((xtr_tensor, ytr_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdPHu2dQwz9A",
        "colab_type": "text"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUH2XyxKwzEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences = True,\n",
        "                                   return_state = True,\n",
        "                                   recurrent_initializer = 'glorot_uniform')\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwEPE_hopwCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# # sample input\n",
        "# example_input_batch, example_target_batch = next(iter(dataset))\n",
        "\n",
        "# sample_hidden = encoder.initialize_hidden_state()\n",
        "# sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "# print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqi1dny0wskN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    query_with_time_axis = tf.expand_dims(query, 1) # (batch_size, hidden size)-->(batch_size, 1, hidden size)\n",
        "\n",
        "    score = self.V(tf.nn.tanh(self.W1(query_with_time_axis)+self.W2(values)))\n",
        "\n",
        "    attention_weights = tf.nn.softmax(score, axis = 1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
        "\n",
        "    return context_vector, attention_weights\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3gcBXSGqCKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# attention_layer = BahdanauAttention(10)\n",
        "# attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "# print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "# print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxW_LL2l2oEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences = True,\n",
        "                                   return_state = True,\n",
        "                                   recurrent_initializer = 'glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "  \n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
        "    output, state = self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2])) # (batch_size * 1, hidden_size)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNSbGGA-qEOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "#                                       sample_hidden, sample_output)\n",
        "\n",
        "# print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6DNGKGK56bS",
        "colab_type": "text"
      },
      "source": [
        "## optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lYhwsNo2u4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBkFHB136udy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp0-aqyU6wbw",
        "colab_type": "text"
      },
      "source": [
        "# Train and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW4n_6PW6vZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def train_step(inp, targ, enc_hidden, acc_object):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([y_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      pred = np.argmax(predictions, axis = 1)\n",
        "      acc_object.update_state(targ[:, t], pred)\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss, acc_object"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5u6u0AY64fV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15f69f6d-4055-4728-9459-f3242d5f1fcd"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  acc = tf.keras.metrics.Accuracy()\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss, acc = train_step(inp, targ, enc_hidden, acc)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 500 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f} Acc {:.4f}'.format(epoch + 1, batch, batch_loss.numpy(), acc.result().numpy()))\n",
        "\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f} Acc {:.4f}\\n'.format(epoch + 1, total_loss / steps_per_epoch, acc.result().numpy()))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.8912 Acc 0.0000\n",
            "Epoch 1 Batch 500 Loss 2.0075 Acc 0.1820\n",
            "Epoch 1 Batch 1000 Loss 1.5110 Acc 0.2213\n",
            "Epoch 1 Batch 1500 Loss 1.3718 Acc 0.2492\n",
            "Epoch 1 Loss 1.6761 Acc 0.2680\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.0275 Acc 0.3401\n",
            "Epoch 2 Batch 500 Loss 0.7942 Acc 0.3776\n",
            "Epoch 2 Batch 1000 Loss 0.7373 Acc 0.3896\n",
            "Epoch 2 Batch 1500 Loss 0.6718 Acc 0.3991\n",
            "Epoch 2 Loss 0.7927 Acc 0.4050\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.4443 Acc 0.4447\n",
            "Epoch 3 Batch 500 Loss 0.4065 Acc 0.4516\n",
            "Epoch 3 Batch 1000 Loss 0.4753 Acc 0.4528\n",
            "Epoch 3 Batch 1500 Loss 0.4505 Acc 0.4543\n",
            "Epoch 3 Loss 0.4724 Acc 0.4554\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.2981 Acc 0.5048\n",
            "Epoch 4 Batch 500 Loss 0.3528 Acc 0.4848\n",
            "Epoch 4 Batch 1000 Loss 0.3821 Acc 0.4836\n",
            "Epoch 4 Batch 1500 Loss 0.2831 Acc 0.4823\n",
            "Epoch 4 Loss 0.3221 Acc 0.4821\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.2009 Acc 0.5276\n",
            "Epoch 5 Batch 500 Loss 0.2274 Acc 0.5056\n",
            "Epoch 5 Batch 1000 Loss 0.1987 Acc 0.5037\n",
            "Epoch 5 Batch 1500 Loss 0.3024 Acc 0.5016\n",
            "Epoch 5 Loss 0.2363 Acc 0.5006\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1660 Acc 0.5312\n",
            "Epoch 6 Batch 500 Loss 0.1785 Acc 0.5189\n",
            "Epoch 6 Batch 1000 Loss 0.2000 Acc 0.5162\n",
            "Epoch 6 Batch 1500 Loss 0.1550 Acc 0.5142\n",
            "Epoch 6 Loss 0.1835 Acc 0.5131\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1137 Acc 0.5192\n",
            "Epoch 7 Batch 500 Loss 0.1260 Acc 0.5280\n",
            "Epoch 7 Batch 1000 Loss 0.1106 Acc 0.5259\n",
            "Epoch 7 Batch 1500 Loss 0.1296 Acc 0.5236\n",
            "Epoch 7 Loss 0.1490 Acc 0.5222\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0920 Acc 0.5481\n",
            "Epoch 8 Batch 500 Loss 0.0904 Acc 0.5343\n",
            "Epoch 8 Batch 1000 Loss 0.1314 Acc 0.5315\n",
            "Epoch 8 Batch 1500 Loss 0.1313 Acc 0.5294\n",
            "Epoch 8 Loss 0.1275 Acc 0.5282\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0945 Acc 0.5361\n",
            "Epoch 9 Batch 500 Loss 0.0909 Acc 0.5387\n",
            "Epoch 9 Batch 1000 Loss 0.0918 Acc 0.5361\n",
            "Epoch 9 Batch 1500 Loss 0.1145 Acc 0.5340\n",
            "Epoch 9 Loss 0.1112 Acc 0.5328\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0983 Acc 0.5276\n",
            "Epoch 10 Batch 500 Loss 0.1143 Acc 0.5415\n",
            "Epoch 10 Batch 1000 Loss 0.0925 Acc 0.5400\n",
            "Epoch 10 Batch 1500 Loss 0.1092 Acc 0.5381\n",
            "Epoch 10 Loss 0.0992 Acc 0.5366\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuenLjeypazv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  # sentence = preprocess(sentence)\n",
        "\n",
        "  inputs = [x_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=x_max_length, padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([y_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(y_max_length):\n",
        "    predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += y_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "    if y_tokenizer.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuDrceTHL02Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def translate(sentence):\n",
        "  result, sentence = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  score = sentence_bleu(sentence, result)\n",
        "  print(\"BLEU score:\", score)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEBlyy8BL7z_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "72a33ead-0607-4bd6-a05d-4b55dd1c1c4d"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd294f3a048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEPevGEfL8n5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a1cb595b-9dbf-499c-91aa-62899f83ede5"
      },
      "source": [
        "translate(deu[1000])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> setz dich dorthin . <end>\n",
            "Predicted translation: sit there . <end> \n",
            "BLEU score: 0.9036020036098448\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu4u2vtqPztD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "522d0140-ead7-4505-83b1-9eaa45174d1a"
      },
      "source": [
        "translate(deu[2000])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> bist du in form ? <end>\n",
            "Predicted translation: are you fit ? <end> \n",
            "BLEU score: 0.9146912192286945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzpLBvtWQmZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "99b8069f-c098-4130-eb04-3e2f1cb568ad"
      },
      "source": [
        "translate(deu[160000])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> ich dachte , du wohnst bei deiner familie . <end>\n",
            "Predicted translation: i thought you were family . <end> \n",
            "BLEU score: 0.8529987544592307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S9S3uJKQqrQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "4411415e-dd14-4e24-d4b0-542eca740ce8"
      },
      "source": [
        "translate(deu[200000])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> das buch handelt vom leben im vereinten konigreich . <end>\n",
            "Predicted translation: the book is on the most of the top of the top of the \n",
            "BLEU score: 0.674961814926904\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdcDBS5eVnGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "2df51b27-12dc-49a3-83db-1852131d8c72"
      },
      "source": [
        "translate(deu[210000])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> allmahlich dammerte mir , dass ich ihn falsch verstanden hatte . <end>\n",
            "Predicted translation: i was beginning to get wrong . <end> \n",
            "BLEU score: 0.7546487907970829\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9ALJUHZIu97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}